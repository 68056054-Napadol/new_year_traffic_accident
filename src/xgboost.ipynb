{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d3e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score,\n",
    "                             mean_absolute_percentage_error, explained_variance_score,\n",
    "                             accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             roc_auc_score, roc_curve)\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb37b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ–¥ï¸  CHECKING GPU AVAILABILITY\n",
      "============================================================\n",
      "âœ… GPU is AVAILABLE and will be used for training\n",
      "   Using: CUDA/GPU acceleration\n",
      "   XGBoost version: 3.1.1\n",
      "   GPU Info: NVIDIA GeForce RTX 3060 Laptop GPU, 6144 MiB\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µ GPU à¸à¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ–¥ï¸  CHECKING GPU AVAILABILITY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š CUDA/GPU à¸ªà¸³à¸«à¸£à¸±à¸š XGBoost\n",
    "gpu_available = False\n",
    "try:\n",
    "    # à¸¥à¸­à¸‡à¸ªà¸£à¹‰à¸²à¸‡ XGBoost model à¸”à¹‰à¸§à¸¢ GPU\n",
    "    test_model = xgb.XGBRegressor(tree_method='hist', gpu_id=0)\n",
    "    gpu_available = True\n",
    "    print(\"âœ… GPU is AVAILABLE and will be used for training\")\n",
    "    print(f\"   Using: CUDA/GPU acceleration\")\n",
    "    tree_method = 'hist'  # à¹ƒà¸Šà¹‰ GPU\n",
    "except:\n",
    "    print(\"âš ï¸  GPU is NOT available, using CPU instead\")\n",
    "    print(f\"   Using: CPU (hist method)\")\n",
    "    tree_method = 'hist'  # à¹ƒà¸Šà¹‰ CPU à¹à¸šà¸šà¹€à¸£à¹‡à¸§\n",
    "\n",
    "# à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ XGBoost version\n",
    "print(f\"   XGBoost version: {xgb.__version__}\")\n",
    "\n",
    "# à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸”à¸¹à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸” GPU à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ (à¸•à¹‰à¸­à¸‡à¸¡à¸µ nvidia-smi)\n",
    "try:\n",
    "    import subprocess\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], \n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"   GPU Info: {result.stdout.strip()}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d8055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“‚ LOADING AND PREPARING DATA\n",
      "============================================================\n",
      "\n",
      "âœ“ Filtered data: December and January only\n",
      "âœ“ Total rows: 172,935\n",
      "âœ“ Unique groups: 952\n",
      "âœ“ Date range: 2018-01-01 00:00:00 to 2025-01-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥\n",
    "# ============================================\n",
    "print(f\"{'='*60}\")\n",
    "print(\"ğŸ“‚ LOADING AND PREPARING DATA\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥\n",
    "df = pd.read_csv('../dataset/accident_count4col.csv')\n",
    "df['adate'] = pd.to_datetime(df['adate'])\n",
    "df['month'] = df['adate'].dt.month\n",
    "\n",
    "# à¸à¸£à¸­à¸‡à¹€à¸‰à¸à¸²à¸°à¹€à¸”à¸·à¸­à¸™ à¸˜à¸±à¸™à¸§à¸²à¸„à¸¡ (12) à¹à¸¥à¸° à¸¡à¸à¸£à¸²à¸„à¸¡ (1)\n",
    "df = df[df['month'].isin([12, 1])].copy()\n",
    "print(f\"âœ“ Filtered data: December and January only\")\n",
    "\n",
    "# à¸ªà¸£à¹‰à¸²à¸‡ group_id à¸ˆà¸²à¸à¸à¸²à¸£à¸£à¸§à¸¡ rcode + aampur_clean + aplace_clean\n",
    "df['group_id'] = (df['rcode'].astype(str) + '_' + \n",
    "                  df['aampur_clean'].astype(str) + '_' + \n",
    "                  df['aplace_clean'].astype(str))\n",
    "\n",
    "print(f\"âœ“ Total rows: {len(df):,}\")\n",
    "print(f\"âœ“ Unique groups: {df['group_id'].nunique()}\")\n",
    "print(f\"âœ“ Date range: {df['adate'].min()} to {df['adate'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c9daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ”§ FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "âœ“ Basic time features created\n",
      "âœ“ Group statistics features created\n",
      "âœ“ Lag features created (1-year, 2-year)\n",
      "âœ“ Rolling window features created (7-day, 14-day)\n",
      "âœ“ Missing values filled\n",
      "âœ“ Total features created: 18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# Feature Engineering (à¸ªà¸£à¹‰à¸²à¸‡à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡)\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ”§ FEATURE ENGINEERING\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¸à¸·à¹‰à¸™à¸à¸²à¸™à¸ˆà¸²à¸à¸§à¸±à¸™à¸—à¸µà¹ˆ\n",
    "df['year'] = df['adate'].dt.year\n",
    "df['day_of_month'] = df['adate'].dt.day\n",
    "df['day_of_week'] = df['adate'].dt.dayofweek  # 0=à¸ˆà¸±à¸™à¸—à¸£à¹Œ, 6=à¸­à¸²à¸—à¸´à¸•à¸¢à¹Œ\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)  # à¹€à¸ªà¸²à¸£à¹Œ-à¸­à¸²à¸—à¸´à¸•à¸¢à¹Œ\n",
    "df['is_december'] = (df['month'] == 12).astype(int)  # à¹€à¸”à¸·à¸­à¸™à¸˜à¸±à¸™à¸§à¸²à¸„à¸¡à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ\n",
    "\n",
    "print(f\"âœ“ Basic time features created\")\n",
    "\n",
    "# à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¸¥à¸¸à¹ˆà¸¡ (Group-level features)\n",
    "# à¸„à¸³à¸™à¸§à¸“à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸°à¸à¸¥à¸¸à¹ˆà¸¡à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆ\n",
    "group_means = df.groupby('group_id')['acc_cases'].mean()\n",
    "df['group_mean'] = df['group_id'].map(group_means)\n",
    "\n",
    "# à¸„à¸³à¸™à¸§à¸“à¸„à¹ˆà¸²à¹€à¸šà¸µà¹ˆà¸¢à¸‡à¹€à¸šà¸™à¸¡à¸²à¸•à¸£à¸à¸²à¸™à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸°à¸à¸¥à¸¸à¹ˆà¸¡\n",
    "group_std = df.groupby('group_id')['acc_cases'].std().fillna(0)\n",
    "df['group_std'] = df['group_id'].map(group_std)\n",
    "\n",
    "print(f\"âœ“ Group statistics features created\")\n",
    "\n",
    "# Lag Features (à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¢à¹‰à¸­à¸™à¸«à¸¥à¸±à¸‡)\n",
    "# à¹€à¸£à¸µà¸¢à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸²à¸¡à¸à¸¥à¸¸à¹ˆà¸¡à¹à¸¥à¸°à¸§à¸±à¸™à¸—à¸µà¹ˆ\n",
    "df = df.sort_values(['group_id', 'adate'])\n",
    "\n",
    "# à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¢à¹‰à¸­à¸™à¸«à¸¥à¸±à¸‡ 1 à¸›à¸µ (365 à¸§à¸±à¸™)\n",
    "df['lag_1year'] = df.groupby('group_id')['acc_cases'].shift(365)\n",
    "\n",
    "# à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¢à¹‰à¸­à¸™à¸«à¸¥à¸±à¸‡ 2 à¸›à¸µ (730 à¸§à¸±à¸™)\n",
    "df['lag_2year'] = df.groupby('group_id')['acc_cases'].shift(730)\n",
    "\n",
    "print(f\"âœ“ Lag features created (1-year, 2-year)\")\n",
    "\n",
    "# Rolling Features (à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™à¸—à¸µà¹ˆ)\n",
    "# à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸‚à¸­à¸‡ 7 à¸§à¸±à¸™à¸—à¸µà¹ˆà¸œà¹ˆà¸²à¸™à¸¡à¸²\n",
    "df['rolling_mean_7d'] = df.groupby('group_id')['acc_cases'].rolling(7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸‚à¸­à¸‡ 14 à¸§à¸±à¸™à¸—à¸µà¹ˆà¸œà¹ˆà¸²à¸™à¸¡à¸²\n",
    "df['rolling_mean_14d'] = df.groupby('group_id')['acc_cases'].rolling(14, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "print(f\"âœ“ Rolling window features created (7-day, 14-day)\")\n",
    "\n",
    "# à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸² NaN à¸”à¹‰à¸§à¸¢ 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "print(f\"âœ“ Missing values filled\")\n",
    "print(f\"âœ“ Total features created: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a87142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ‚ï¸  SPLITTING DATA\n",
      "============================================================\n",
      "\n",
      "âœ“ Train set: 138,438 rows (2018-01-01 00:00:00 to 2024-01-31 00:00:00)\n",
      "âœ“ Test set:  34,497 rows (2024-12-01 00:00:00 to 2025-01-31 00:00:00)\n",
      "âœ“ Features: 15 columns\n",
      "âœ“ Target: acc_cases\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================\n",
    "# à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Train/Test\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"âœ‚ï¸  SPLITTING DATA\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Train: à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¹ˆà¸­à¸™ 2024-01-31\n",
    "train = df[df['adate'] <= '2024-01-31'].copy()\n",
    "\n",
    "# Test: à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ 2024-12-01 à¸–à¸¶à¸‡ 2025-01-31\n",
    "test = df[(df['adate'] >= '2024-12-01') & (df['adate'] <= '2025-01-31')].copy()\n",
    "\n",
    "print(f\"âœ“ Train set: {len(train):,} rows ({train['adate'].min()} to {train['adate'].max()})\")\n",
    "print(f\"âœ“ Test set:  {len(test):,} rows ({test['adate'].min()} to {test['adate'].max()})\")\n",
    "\n",
    "# à¸à¸³à¸«à¸™à¸” Feature columns (à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸—à¸µà¹ˆà¸ˆà¸°à¹ƒà¸Šà¹‰à¹ƒà¸™à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢)\n",
    "feature_cols = [\n",
    "    'month', 'day_of_month', 'day_of_week', 'is_weekend', \n",
    "    'is_december', 'year', 'rcode', 'aampur_clean', 'aplace_clean',\n",
    "    'group_mean', 'group_std', 'lag_1year', 'lag_2year', \n",
    "    'rolling_mean_7d', 'rolling_mean_14d'\n",
    "]\n",
    "\n",
    "# à¹à¸¢à¸ Features (X) à¹à¸¥à¸° Target (y)\n",
    "X_train = train[feature_cols]\n",
    "y_train = train['acc_cases']\n",
    "X_test = test[feature_cols]\n",
    "y_test = test['acc_cases']\n",
    "\n",
    "print(f\"âœ“ Features: {len(feature_cols)} columns\")\n",
    "print(f\"âœ“ Target: acc_cases\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c20b2fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ¤– TRAINING XGBOOST MODEL\n",
      "============================================================\n",
      "\n",
      "Model Configuration:\n",
      "  â€¢ Tree method: hist (GPU âš¡)\n",
      "  â€¢ Number of trees: 200\n",
      "  â€¢ Learning rate: 0.05\n",
      "  â€¢ Max depth: 6\n",
      "\n",
      "Training model... (This may take a while)\n",
      "[0]\tvalidation_0-rmse:4.33900\n",
      "[50]\tvalidation_0-rmse:2.57273\n",
      "[100]\tvalidation_0-rmse:2.53758\n",
      "[150]\tvalidation_0-rmse:2.54101\n",
      "[199]\tvalidation_0-rmse:2.54717\n",
      "\n",
      "âœ… Training completed!\n",
      "   Training time: 0.90 seconds (0.02 minutes)\n",
      "   Speed: 152990 samples/second\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# à¸ªà¸£à¹‰à¸²à¸‡à¹à¸¥à¸°à¹€à¸—à¸£à¸™à¹‚à¸¡à¹€à¸”à¸¥ XGBoost\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ¤– TRAINING XGBOOST MODEL\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸¡à¹€à¸”à¸¥ XGBoost à¸à¸£à¹‰à¸­à¸¡à¸à¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œà¸—à¸µà¹ˆà¸›à¸£à¸±à¸šà¹à¸•à¹ˆà¸‡\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=200,          # à¸ˆà¸³à¸™à¸§à¸™ trees (à¸¢à¸´à¹ˆà¸‡à¸¡à¸²à¸à¸¢à¸´à¹ˆà¸‡à¹à¸¡à¹ˆà¸™à¹à¸•à¹ˆà¸Šà¹‰à¸²)\n",
    "    learning_rate=0.05,        # à¸­à¸±à¸•à¸£à¸²à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰ (à¸•à¹ˆà¸³ = à¸Šà¹‰à¸²à¹à¸•à¹ˆà¹à¸¡à¹ˆà¸™à¸à¸§à¹ˆà¸²)\n",
    "    max_depth=6,               # à¸„à¸§à¸²à¸¡à¸¥à¸¶à¸à¸ªà¸¹à¸‡à¸ªà¸¸à¸”à¸‚à¸­à¸‡ tree\n",
    "    min_child_weight=1,        # à¸™à¹‰à¸³à¸«à¸™à¸±à¸à¸‚à¸±à¹‰à¸™à¸•à¹ˆà¸³à¸‚à¸­à¸‡ leaf node\n",
    "    subsample=0.8,             # à¸ªà¸¸à¹ˆà¸¡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ 80% à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸£à¸­à¸š\n",
    "    colsample_bytree=0.8,      # à¸ªà¸¸à¹ˆà¸¡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œ 80% à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸° tree\n",
    "    tree_method=tree_method,   # à¹ƒà¸Šà¹‰ GPU à¸«à¸£à¸·à¸­ CPU à¸•à¸²à¸¡à¸—à¸µà¹ˆà¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸§à¹‰\n",
    "    device='cuda',              # à¸«à¸£à¸·à¸­ 'cpu'\n",
    "    random_state=42,           # à¸à¸³à¸«à¸™à¸” seed à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸‹à¹‰à¸³à¹„à¸”à¹‰\n",
    "    n_jobs=-1                  # à¹ƒà¸Šà¹‰ CPU à¸—à¸¸à¸ core (à¸ªà¸³à¸«à¸£à¸±à¸š CPU mode)\n",
    ")\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  â€¢ Tree method: {tree_method} {'(GPU âš¡)' if gpu_available else '(CPU)'}\")\n",
    "print(f\"  â€¢ Number of trees: 200\")\n",
    "print(f\"  â€¢ Learning rate: 0.05\")\n",
    "print(f\"  â€¢ Max depth: 6\")\n",
    "print(f\"\\nTraining model... (This may take a while)\")\n",
    "\n",
    "# à¹€à¸—à¸£à¸™à¹‚à¸¡à¹€à¸”à¸¥\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],  # à¹ƒà¸Šà¹‰ test set à¸ªà¸³à¸«à¸£à¸±à¸š validation\n",
    "    verbose=50                     # à¹à¸ªà¸”à¸‡à¸œà¸¥à¸—à¸¸à¸ 50 iterations\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Training completed!\")\n",
    "print(f\"   Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"   Speed: {len(train)/training_time:.0f} samples/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27e326cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ’¾ SAVING MODEL\n",
      "============================================================\n",
      "\n",
      "âœ“ Saved: models/xgboost_accident_model.json\n",
      "âœ“ Saved: models/xgboost_accident_model.pkl\n",
      "âœ“ Saved: models/model_info.json\n",
      "\n",
      "ğŸ“‚ Model files saved in 'models/' folder\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ğŸ’¾ SAVE MODEL (à¸šà¸±à¸™à¸—à¸¶à¸à¹‚à¸¡à¹€à¸”à¸¥)\n",
    "# ============================================\n",
    "print(f\"{'='*60}\")\n",
    "print(\"ğŸ’¾ SAVING MODEL\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸šà¹‚à¸¡à¹€à¸”à¸¥\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# à¸šà¸±à¸™à¸—à¸¶à¸à¹‚à¸¡à¹€à¸”à¸¥ XGBoost (2 à¸§à¸´à¸˜à¸µ)\n",
    "\n",
    "# à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 1: XGBoost JSON format (à¹à¸™à¸°à¸™à¸³ - à¸£à¸­à¸‡à¸£à¸±à¸šà¸‚à¹‰à¸²à¸¡ platform)\n",
    "model_path_json = 'models/xgboost_accident_model.json'\n",
    "model.save_model(model_path_json)\n",
    "print(f\"âœ“ Saved: {model_path_json}\")\n",
    "\n",
    "# à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 2: Python Pickle format (à¸ªà¸³à¸«à¸£à¸±à¸šà¹ƒà¸Šà¹‰à¹ƒà¸™ Python à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™)\n",
    "model_path_pkl = 'models/xgboost_accident_model.pkl'\n",
    "with open(model_path_pkl, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"âœ“ Saved: {model_path_pkl}\")\n",
    "\n",
    "# à¸šà¸±à¸™à¸—à¸¶à¸ feature columns (à¸ªà¸³à¸„à¸±à¸!)\n",
    "feature_info = {\n",
    "    'feature_columns': feature_cols,\n",
    "    'model_type': 'XGBRegressor',\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'device': 'gpu',\n",
    "    'training_samples': len(train),\n",
    "    'xgboost_version': xgb.__version__\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('models/model_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "print(f\"âœ“ Saved: models/model_info.json\")\n",
    "\n",
    "print(f\"\\nğŸ“‚ Model files saved in 'models/' folder\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d826d3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š MODEL EVALUATION - COMPREHENSIVE METRICS\n",
      "============================================================\n",
      "\n",
      "Making predictions...\n",
      "âœ“ Prediction time: 0.01 seconds\n",
      "âœ“ Speed: 2364617 predictions/second\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================\n",
    "# COMPREHENSIVE MODEL EVALUATION\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“Š MODEL EVALUATION - COMPREHENSIVE METRICS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# à¸—à¸³à¸™à¸²à¸¢à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ\n",
    "print(f\"\\nMaking predictions...\")\n",
    "start_pred = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "pred_time = time.time() - start_pred\n",
    "\n",
    "y_pred = np.maximum(0, y_pred)  # à¹„à¸¡à¹ˆà¹ƒà¸«à¹‰à¸„à¹ˆà¸²à¸•à¸´à¸”à¸¥à¸š (à¸ˆà¸³à¸™à¸§à¸™à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸à¸•à¹‰à¸­à¸‡ >= 0)\n",
    "\n",
    "print(f\"âœ“ Prediction time: {pred_time:.2f} seconds\")\n",
    "print(f\"âœ“ Speed: {len(test)/pred_time:.0f} predictions/second\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec89b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š REGRESSION METRICS:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  MAE (Mean Absolute Error):          1.7143\n",
      "    â†’ à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸‚à¸­à¸‡à¸„à¸§à¸²à¸¡à¸„à¸¥à¸²à¸”à¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™\n",
      "  MSE (Mean Squared Error):           6.4881\n",
      "  RMSE (Root Mean Squared Error):     2.5472\n",
      "    â†’ à¹ƒà¸«à¹‰à¸™à¹‰à¸³à¸«à¸™à¸±à¸à¸à¸±à¸šà¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸¡à¸²à¸\n",
      "  MAPE (Mean Absolute % Error):       63.86%\n",
      "    â†’ à¸„à¸§à¸²à¸¡à¸„à¸¥à¸²à¸”à¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™à¹€à¸›à¹‡à¸™à¹€à¸›à¸­à¸£à¹Œà¹€à¸‹à¹‡à¸™à¸•à¹Œ\n",
      "  RÂ² Score:                           0.6736\n",
      "    â†’ à¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸™à¸à¸²à¸£à¸­à¸˜à¸´à¸šà¸²à¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (à¸¢à¸´à¹ˆà¸‡à¹ƒà¸à¸¥à¹‰ 1 à¸¢à¸´à¹ˆà¸‡à¸”à¸µ)\n",
      "  Explained Variance:                 0.6736\n",
      "  Median Absolute Error:              1.1748\n",
      "  Max Error:                          39.9129\n",
      "\n",
      "ğŸ“ˆ PREDICTION DISTRIBUTION:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Actual Total Cases:         144,302\n",
      "  Predicted Total Cases:      143,679\n",
      "  Difference:                    -623\n",
      "  Total Accuracy:               99.57%\n",
      "\n",
      "  Actual Statistics:\n",
      "    Min:     1.00\n",
      "    25%:     1.00\n",
      "    Median:  3.00\n",
      "    Mean:    4.18\n",
      "    75%:     5.00\n",
      "    Max:     70.00\n",
      "\n",
      "  Predicted Statistics:\n",
      "    Min:     0.60\n",
      "    25%:     2.13\n",
      "    Median:  3.06\n",
      "    Mean:    4.16\n",
      "    75%:     4.70\n",
      "    Max:     59.19\n",
      "\n",
      "ğŸ˜ï¸  PER-GROUP EVALUATION:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Total Groups Evaluated:  924\n",
      "  Average MAE:             1.4508\n",
      "  Average RMSE:            1.8979\n",
      "  Average RÂ²:              0.1483\n",
      "  Best MAE:                0.0068\n",
      "  Worst MAE:               7.1688\n",
      "\n",
      "  âœ“ Saved to: evaluation_metrics_per_group.csv\n",
      "\n",
      "âŒ TOP 10 WORST PERFORMING GROUPS:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      " group_id      mae      rmse        r2  actual_sum  predicted_sum\n",
      "8301_1_83 7.168767 10.221081  0.497532        1574    1478.829468\n",
      "5701_1_57 6.116611  7.408036 -0.282829        1443    1521.716797\n",
      "5001_1_50 5.849039  9.012830  0.556476         421     381.070312\n",
      "4001_1_40 5.760258  7.130275  0.118881        1730    1771.958740\n",
      "2004_4_20 5.696636  7.506165 -0.308573        1581    1472.426270\n",
      "3201_1_32 5.177659  6.483794  0.299158         183     231.280640\n",
      "5201_1_52 5.121208  7.302083  0.143915         632     648.557495\n",
      "8302_2_83 4.936647  6.319690  0.347842         723     664.847229\n",
      "8303_3_83 4.808777  6.120865  0.192689         895     870.038208\n",
      "3001_1_30 4.780340  7.248026  0.487004        1135    1078.165161\n",
      "\n",
      "âœ… TOP 10 BEST PERFORMING GROUPS:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  group_id      mae     rmse  r2  actual_sum  predicted_sum\n",
      "1039_39_10 0.006794 0.006794 0.0           1       1.006794\n",
      "1018_18_10 0.009117 0.009117 0.0           1       1.009117\n",
      "1020_20_10 0.028054 0.028054 0.0           1       0.971946\n",
      "1034_34_10 0.033094 0.045591 0.0           2       1.937285\n",
      "1022_22_10 0.037594 0.041680 0.0           2       2.075187\n",
      "3311_11_33 0.038816 0.038816 0.0           1       0.961184\n",
      "1019_19_10 0.063611 0.075385 0.0           6       6.156378\n",
      "1010_10_10 0.071228 0.085549 0.0           2       2.142456\n",
      "1041_41_10 0.086777 0.127495 0.0           6       6.106848\n",
      "1044_44_10 0.093854 0.093854 0.0           1       1.093854\n",
      "\n",
      "ğŸ” FEATURE IMPORTANCE:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  (à¹à¸ªà¸”à¸‡à¸§à¹ˆà¸²à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¹„à¸«à¸™à¸¡à¸µà¸œà¸¥à¸•à¹ˆà¸­à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸¡à¸²à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”)\n",
      "         feature  importance\n",
      " rolling_mean_7d    0.571894\n",
      "rolling_mean_14d    0.193037\n",
      "      group_mean    0.045652\n",
      "           month    0.029519\n",
      "     is_december    0.026564\n",
      "    day_of_month    0.025700\n",
      "            year    0.018975\n",
      "       group_std    0.015603\n",
      "      is_weekend    0.014976\n",
      "           rcode    0.013556\n",
      "     day_of_week    0.012877\n",
      "    aampur_clean    0.012609\n",
      "    aplace_clean    0.011659\n",
      "       lag_1year    0.007379\n",
      "       lag_2year    0.000000\n",
      "\n",
      "  âœ“ Saved to: feature_importance.csv\n",
      "\n",
      "ğŸ“Š GENERATING VISUALIZATIONS:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  âœ“ Saved: evaluation_scatter_plot.png\n",
      "  âœ“ Saved: evaluation_residuals.png\n",
      "  âœ“ Saved: evaluation_feature_importance.png\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# REGRESSION METRICS (à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸³à¸™à¸²à¸¢à¸ˆà¸³à¸™à¸§à¸™à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸)\n",
    "# ============================================\n",
    "print(f\"\\nğŸ“Š REGRESSION METRICS:\")\n",
    "print(f\"{'â”€'*60}\")\n",
    "\n",
    "# à¸„à¸³à¸™à¸§à¸“ Metrics à¸à¸·à¹‰à¸™à¸à¸²à¸™\n",
    "mae = mean_absolute_error(y_test, y_pred)  # à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸ªà¸±à¸¡à¸šà¸¹à¸£à¸“à¹Œ\n",
    "mse = mean_squared_error(y_test, y_pred)   # à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸à¸³à¸¥à¸±à¸‡à¸ªà¸­à¸‡\n",
    "rmse = np.sqrt(mse)                        # à¸£à¸²à¸à¸—à¸µà¹ˆà¸ªà¸­à¸‡à¸‚à¸­à¸‡ MSE\n",
    "r2 = r2_score(y_test, y_pred)              # à¸ªà¸±à¸¡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¹Œà¸à¸²à¸£à¸à¸³à¸«à¸™à¸” (0-1)\n",
    "\n",
    "# MAPE - à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸à¸²à¸£à¸«à¸²à¸£à¸”à¹‰à¸§à¸¢ 0\n",
    "mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test == 0, 1, y_test))) * 100\n",
    "\n",
    "# Metrics à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡\n",
    "explained_var = explained_variance_score(y_test, y_pred)  # à¸„à¸§à¸²à¸¡à¹à¸›à¸£à¸›à¸£à¸§à¸™à¸—à¸µà¹ˆà¸­à¸˜à¸´à¸šà¸²à¸¢à¹„à¸”à¹‰\n",
    "median_ae = np.median(np.abs(y_test - y_pred))            # à¸„à¹ˆà¸²à¸¡à¸±à¸˜à¸¢à¸à¸²à¸™à¸‚à¸­à¸‡à¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸à¸¥à¸²à¸”\n",
    "max_error = np.max(np.abs(y_test - y_pred))               # à¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸ªà¸¹à¸‡à¸ªà¸¸à¸”\n",
    "\n",
    "print(f\"  MAE (Mean Absolute Error):          {mae:.4f}\")\n",
    "print(f\"    â†’ à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸‚à¸­à¸‡à¸„à¸§à¸²à¸¡à¸„à¸¥à¸²à¸”à¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™\")\n",
    "print(f\"  MSE (Mean Squared Error):           {mse:.4f}\")\n",
    "print(f\"  RMSE (Root Mean Squared Error):     {rmse:.4f}\")\n",
    "print(f\"    â†’ à¹ƒà¸«à¹‰à¸™à¹‰à¸³à¸«à¸™à¸±à¸à¸à¸±à¸šà¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸¡à¸²à¸\")\n",
    "print(f\"  MAPE (Mean Absolute % Error):       {mape:.2f}%\")\n",
    "print(f\"    â†’ à¸„à¸§à¸²à¸¡à¸„à¸¥à¸²à¸”à¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™à¹€à¸›à¹‡à¸™à¹€à¸›à¸­à¸£à¹Œà¹€à¸‹à¹‡à¸™à¸•à¹Œ\")\n",
    "print(f\"  RÂ² Score:                           {r2:.4f}\")\n",
    "print(f\"    â†’ à¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸™à¸à¸²à¸£à¸­à¸˜à¸´à¸šà¸²à¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (à¸¢à¸´à¹ˆà¸‡à¹ƒà¸à¸¥à¹‰ 1 à¸¢à¸´à¹ˆà¸‡à¸”à¸µ)\")\n",
    "print(f\"  Explained Variance:                 {explained_var:.4f}\")\n",
    "print(f\"  Median Absolute Error:              {median_ae:.4f}\")\n",
    "print(f\"  Max Error:                          {max_error:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# PREDICTION DISTRIBUTION ANALYSIS\n",
    "# ============================================\n",
    "print(f\"\\nğŸ“ˆ PREDICTION DISTRIBUTION:\")\n",
    "print(f\"{'â”€'*60}\")\n",
    "print(f\"  Actual Total Cases:      {y_test.sum():>10,.0f}\")\n",
    "print(f\"  Predicted Total Cases:   {y_pred.sum():>10,.0f}\")\n",
    "print(f\"  Difference:              {(y_pred.sum() - y_test.sum()):>10,.0f}\")\n",
    "print(f\"  Total Accuracy:          {(1 - abs(y_pred.sum() - y_test.sum()) / y_test.sum()) * 100:>10.2f}%\")\n",
    "\n",
    "print(f\"\\n  Actual Statistics:\")\n",
    "print(f\"    Min:     {y_test.min():.2f}\")\n",
    "print(f\"    25%:     {np.percentile(y_test, 25):.2f}\")\n",
    "print(f\"    Median:  {np.median(y_test):.2f}\")\n",
    "print(f\"    Mean:    {y_test.mean():.2f}\")\n",
    "print(f\"    75%:     {np.percentile(y_test, 75):.2f}\")\n",
    "print(f\"    Max:     {y_test.max():.2f}\")\n",
    "\n",
    "print(f\"\\n  Predicted Statistics:\")\n",
    "print(f\"    Min:     {y_pred.min():.2f}\")\n",
    "print(f\"    25%:     {np.percentile(y_pred, 25):.2f}\")\n",
    "print(f\"    Median:  {np.median(y_pred):.2f}\")\n",
    "print(f\"    Mean:    {y_pred.mean():.2f}\")\n",
    "print(f\"    75%:     {np.percentile(y_pred, 75):.2f}\")\n",
    "print(f\"    Max:     {y_pred.max():.2f}\")\n",
    "\n",
    "# ============================================\n",
    "# PER-GROUP EVALUATION (à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¹à¸•à¹ˆà¸¥à¸°à¸à¸¥à¸¸à¹ˆà¸¡à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆ)\n",
    "# ============================================\n",
    "print(f\"\\nğŸ˜ï¸  PER-GROUP EVALUATION:\")\n",
    "print(f\"{'â”€'*60}\")\n",
    "\n",
    "test_copy = test.copy()\n",
    "test_copy['predicted'] = y_pred\n",
    "group_metrics = []\n",
    "\n",
    "# à¸§à¸™à¸¥à¸¹à¸›à¸„à¸³à¸™à¸§à¸“ metrics à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸•à¹ˆà¸¥à¸°à¸à¸¥à¸¸à¹ˆà¸¡\n",
    "for group_id in test_copy['group_id'].unique():\n",
    "    group_test = test_copy[test_copy['group_id'] == group_id]\n",
    "    y_true = group_test['acc_cases']\n",
    "    y_pred_group = group_test['predicted']\n",
    "    \n",
    "    # à¸„à¸³à¸™à¸§à¸“ metrics\n",
    "    group_mae = mean_absolute_error(y_true, y_pred_group)\n",
    "    group_rmse = np.sqrt(mean_squared_error(y_true, y_pred_group))\n",
    "    group_r2 = r2_score(y_true, y_pred_group) if len(y_true) > 1 else 0\n",
    "    \n",
    "    group_metrics.append({\n",
    "        'group_id': group_id,\n",
    "        'mae': group_mae,\n",
    "        'rmse': group_rmse,\n",
    "        'r2': group_r2,\n",
    "        'n_samples': len(group_test),\n",
    "        'actual_sum': y_true.sum(),\n",
    "        'predicted_sum': y_pred_group.sum(),\n",
    "        'accuracy': (1 - abs(y_pred_group.sum() - y_true.sum()) / (y_true.sum() + 1)) * 100\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(group_metrics)\n",
    "\n",
    "print(f\"  Total Groups Evaluated:  {len(metrics_df)}\")\n",
    "print(f\"  Average MAE:             {metrics_df['mae'].mean():.4f}\")\n",
    "print(f\"  Average RMSE:            {metrics_df['rmse'].mean():.4f}\")\n",
    "print(f\"  Average RÂ²:              {metrics_df['r2'].mean():.4f}\")\n",
    "print(f\"  Best MAE:                {metrics_df['mae'].min():.4f}\")\n",
    "print(f\"  Worst MAE:               {metrics_df['mae'].max():.4f}\")\n",
    "\n",
    "# à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸¥à¸°à¹€à¸­à¸µà¸¢à¸”\n",
    "metrics_df.to_csv('evaluation_metrics_per_group.csv', index=False)\n",
    "print(f\"\\n  âœ“ Saved to: evaluation_metrics_per_group.csv\")\n",
    "\n",
    "# à¹à¸ªà¸”à¸‡ 10 à¸à¸¥à¸¸à¹ˆà¸¡à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢à¹„à¸”à¹‰à¹à¸¢à¹ˆà¸—à¸µà¹ˆà¸ªà¸¸à¸”\n",
    "print(f\"\\nâŒ TOP 10 WORST PERFORMING GROUPS:\")\n",
    "print(f\"{'â”€'*60}\")\n",
    "worst = metrics_df.nlargest(10, 'mae')[['group_id', 'mae', 'rmse', 'r2', 'actual_sum', 'predicted_sum']]\n",
    "print(worst.to_string(index=False))\n",
    "\n",
    "# à¹à¸ªà¸”à¸‡ 10 à¸à¸¥à¸¸à¹ˆà¸¡à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢à¹„à¸”à¹‰à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”\n",
    "print(f\"\\nâœ… TOP 10 BEST PERFORMING GROUPS:\")\n",
    "print(f\"{'â”€'*60}\")\n",
    "best = metrics_df.nsmallest(10, 'mae')[['group_id', 'mae', 'rmse', 'r2', 'actual_sum', 'predicted_sum']]\n",
    "print(best.to_string(index=False))\n",
    "\n",
    "# ============================================\n",
    "# FEATURE IMPORTANCE (à¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸°à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œ)\n",
    "# ============================================\n",
    "print(f\"\\nğŸ” FEATURE IMPORTANCE:\")\n",
    "print(f\"{'â”€'*60}\")\n",
    "print(f\"  (à¹à¸ªà¸”à¸‡à¸§à¹ˆà¸²à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¹„à¸«à¸™à¸¡à¸µà¸œà¸¥à¸•à¹ˆà¸­à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸¡à¸²à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”)\")\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance.to_string(index=False))\n",
    "importance.to_csv('feature_importance.csv', index=False)\n",
    "print(f\"\\n  âœ“ Saved to: feature_importance.csv\")\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATION (à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿà¹à¸ªà¸”à¸‡à¸œà¸¥)\n",
    "# ============================================\n",
    "print(f\"\\nğŸ“Š GENERATING VISUALIZATIONS:\")\n",
    "print(f\"{'â”€'*60}\")\n",
    "\n",
    "# 1. Actual vs Predicted Scatter Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, s=10)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Cases', fontsize=12) # (à¸ˆà¸³à¸™à¸§à¸™à¸ˆà¸£à¸´à¸‡)\n",
    "plt.ylabel('Predicted Cases', fontsize=12) # (à¸ˆà¸³à¸™à¸§à¸™à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢)\n",
    "plt.title('Actual vs Predicted Cases', fontsize=14) #\\n(à¸ˆà¸¸à¸”à¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆà¹ƒà¸à¸¥à¹‰à¹€à¸ªà¹‰à¸™à¹à¸”à¸‡ = à¸—à¸³à¸™à¸²à¸¢à¹à¸¡à¹ˆà¸™à¸¢à¸³)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_scatter_plot.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"  âœ“ Saved: evaluation_scatter_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# 2. Residuals Plot (à¸à¸£à¸²à¸Ÿà¹à¸ªà¸”à¸‡à¸„à¸§à¸²à¸¡à¸„à¸¥à¸²à¸”à¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™)\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals, alpha=0.5, s=10)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted Cases', fontsize=12) #(à¸ˆà¸³à¸™à¸§à¸™à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢)\n",
    "plt.ylabel('Residuals', fontsize=12) #  (à¸„à¸§à¸²à¸¡à¸„à¸¥à¸²à¸”à¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™)\n",
    "plt.title('Residual Plot)', fontsize=14) #\\n(à¸ˆà¸¸à¸”à¸„à¸§à¸£à¸à¸£à¸°à¸ˆà¸²à¸¢à¸£à¸­à¸šà¹€à¸ªà¹‰à¸™ 0 = à¹‚à¸¡à¹€à¸”à¸¥à¸”à¸µ\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_residuals.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"  âœ“ Saved: evaluation_residuals.png\")\n",
    "plt.close()\n",
    "\n",
    "# 3. Feature Importance Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_n = 15\n",
    "importance_top = importance.head(top_n)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(importance_top)))\n",
    "plt.barh(range(len(importance_top)), importance_top['importance'], color=colors)\n",
    "plt.yticks(range(len(importance_top)), importance_top['feature'])\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.title(f'Top {top_n} Feature Importance\\n(à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”)', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"  âœ“ Saved: evaluation_feature_importance.png\")\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3efcd616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š MODEL EVALUATION\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ TIME-SERIES FORECAST METRICS:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  MAE (Mean Absolute Error):     1.7143\n",
      "    â†’ à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸œà¸´à¸”à¸à¸¥à¸²à¸” Â±1.71 cases/day\n",
      "\n",
      "  RMSE (Root Mean Squared Error): 2.5472\n",
      "    â†’ à¹€à¸™à¹‰à¸™à¸¥à¸‡à¹‚à¸—à¸©à¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹ƒà¸«à¸à¹ˆ\n",
      "\n",
      "  RÂ² Score:                       0.6736\n",
      "    â†’ à¸­à¸˜à¸´à¸šà¸²à¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸”à¹‰ 67.4%\n",
      "\n",
      "  MAPE (Mean Absolute % Error):   63.86%\n",
      "    â†’ à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸œà¸´à¸”à¸à¸¥à¸²à¸” 63.9%\n",
      "\n",
      "ğŸ’¡ INTERPRETATION:\n",
      "  â­â­ Good prediction (RÂ² > 0.6)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š ACTUAL vs PREDICTED TOTALS:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Actual Total:         144,302 cases\n",
      "  Predicted Total:      143,679 cases\n",
      "  Difference:              -623 cases\n",
      "  Accuracy:                99.6%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ˜ï¸  PER-GROUP PERFORMANCE:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Total groups evaluated: 924\n",
      "  Average MAE per group:  1.45\n",
      "  Best MAE:               0.01\n",
      "  Worst MAE:              7.17\n",
      "\n",
      "  âš ï¸  TOP 5 WORST PERFORMING GROUPS:\n",
      "     8301_1_83: MAE=7.17, Actual=1574, Pred=1479\n",
      "     5701_1_57: MAE=6.12, Actual=1443, Pred=1522\n",
      "     5001_1_50: MAE=5.85, Actual=421, Pred=381\n",
      "     4001_1_40: MAE=5.76, Actual=1730, Pred=1772\n",
      "     2004_4_20: MAE=5.70, Actual=1581, Pred=1472\n",
      "\n",
      "  âœ… TOP 5 BEST PERFORMING GROUPS:\n",
      "     1018_18_10: MAE=0.01, Actual=1, Pred=1\n",
      "     1039_39_10: MAE=0.01, Actual=1, Pred=1\n",
      "     1020_20_10: MAE=0.03, Actual=1, Pred=1\n",
      "     1034_34_10: MAE=0.03, Actual=2, Pred=2\n",
      "     1022_22_10: MAE=0.04, Actual=2, Pred=2\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ” TOP 10 IMPORTANT FEATURES:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  rolling_mean_7d      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.5719\n",
      "  rolling_mean_14d     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.1930\n",
      "  group_mean           â–ˆâ–ˆ 0.0457\n",
      "  month                â–ˆ 0.0295\n",
      "  is_december          â–ˆ 0.0266\n",
      "  day_of_month         â–ˆ 0.0257\n",
      "  year                  0.0190\n",
      "  group_std             0.0156\n",
      "  is_weekend            0.0150\n",
      "  rcode                 0.0136\n",
      "\n",
      "âœ“ Saved: models/evaluation_per_group.csv\n",
      "âœ“ Saved: models/feature_importance.csv\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ EVALUATION SUMMARY\n",
      "============================================================\n",
      "  âœ“ Model Type:      XGBoost Regressor\n",
      "  âœ“ Training Device: GPU\n",
      "  âœ“ Training Time:   0.90 seconds\n",
      "  âœ“ MAE:             1.7143\n",
      "  âœ“ RMSE:            2.5472\n",
      "  âœ“ RÂ²:              0.6736\n",
      "  âœ“ MAPE:            63.86%\n",
      "  âœ“ Total Accuracy:  99.6%\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MODEL EVALUATION (à¹à¸šà¸šà¸‡à¹ˆà¸²à¸¢ - KISS)\n",
    "# ============================================\n",
    "print(f\"{'='*60}\")\n",
    "print(\"ğŸ“Š MODEL EVALUATION\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# à¸—à¸³à¸™à¸²à¸¢\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.maximum(0, y_pred)  # à¹„à¸¡à¹ˆà¹ƒà¸«à¹‰à¸•à¸´à¸”à¸¥à¸š\n",
    "\n",
    "# ============================================\n",
    "# 1. Regression Metrics (à¸ªà¸³à¸«à¸£à¸±à¸š Time-Series Forecast)\n",
    "# ============================================\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test == 0, 1, y_test))) * 100\n",
    "\n",
    "print(\"ğŸ“ˆ TIME-SERIES FORECAST METRICS:\")\n",
    "print(f\"{'â”€'*60}\")\n",
    "print(f\"  MAE (Mean Absolute Error):     {mae:.4f}\")\n",
    "print(f\"    â†’ à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸œà¸´à¸”à¸à¸¥à¸²à¸” Â±{mae:.2f} cases/day\")\n",
    "print(f\"\\n  RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
    "print(f\"    â†’ à¹€à¸™à¹‰à¸™à¸¥à¸‡à¹‚à¸—à¸©à¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹ƒà¸«à¸à¹ˆ\")\n",
    "print(f\"\\n  RÂ² Score:                       {r2:.4f}\")\n",
    "print(f\"    â†’ à¸­à¸˜à¸´à¸šà¸²à¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸”à¹‰ {r2*100:.1f}%\")\n",
    "print(f\"\\n  MAPE (Mean Absolute % Error):   {mape:.2f}%\")\n",
    "print(f\"    â†’ à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸œà¸´à¸”à¸à¸¥à¸²à¸” {mape:.1f}%\")\n",
    "\n",
    "# à¸•à¸µà¸„à¸§à¸²à¸¡à¸œà¸¥\n",
    "print(f\"\\nğŸ’¡ INTERPRETATION:\")\n",
    "if r2 > 0.8:\n",
    "    print(f\"  â­â­â­ Excellent prediction (RÂ² > 0.8)\")\n",
    "elif r2 > 0.6:\n",
    "    print(f\"  â­â­ Good prediction (RÂ² > 0.6)\")\n",
    "elif r2 > 0.4:\n",
    "    print(f\"  â­ Moderate prediction (RÂ² > 0.4)\")\n",
    "else:\n",
    "    print(f\"  âš ï¸  Poor prediction (RÂ² < 0.4)\")\n",
    "\n",
    "# ============================================\n",
    "# 2. Actual vs Predicted Comparison\n",
    "# ============================================\n",
    "print(f\"\\n{'â”€'*60}\")\n",
    "print(f\"ğŸ“Š ACTUAL vs PREDICTED TOTALS:\")\n",
    "print(f\"{'â”€'*60}\")\n",
    "print(f\"  Actual Total:      {y_test.sum():>10,.0f} cases\")\n",
    "print(f\"  Predicted Total:   {y_pred.sum():>10,.0f} cases\")\n",
    "print(f\"  Difference:        {(y_pred.sum() - y_test.sum()):>10,.0f} cases\")\n",
    "print(f\"  Accuracy:          {(1 - abs(y_pred.sum() - y_test.sum()) / y_test.sum()) * 100:>10.1f}%\")\n",
    "\n",
    "# ============================================\n",
    "# 3. Per-Group Evaluation (à¸à¸¥à¸¸à¹ˆà¸¡à¹„à¸«à¸™à¸—à¸³à¸™à¸²à¸¢à¹„à¸”à¹‰à¸”à¸µ/à¹à¸¢à¹ˆ)\n",
    "# ============================================\n",
    "print(f\"\\n{'â”€'*60}\")\n",
    "print(f\"ğŸ˜ï¸  PER-GROUP PERFORMANCE:\")\n",
    "print(f\"{'â”€'*60}\")\n",
    "\n",
    "test_eval = test.copy()\n",
    "test_eval['predicted'] = y_pred\n",
    "test_eval['error'] = np.abs(test_eval['acc_cases'] - test_eval['predicted'])\n",
    "\n",
    "group_metrics = test_eval.groupby('group_id').agg({\n",
    "    'acc_cases': ['sum', 'mean'],\n",
    "    'predicted': ['sum', 'mean'],\n",
    "    'error': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "group_metrics.columns = ['actual_sum', 'actual_mean', 'pred_sum', 'pred_mean', 'mae']\n",
    "group_metrics = group_metrics.reset_index()\n",
    "\n",
    "print(f\"  Total groups evaluated: {len(group_metrics)}\")\n",
    "print(f\"  Average MAE per group:  {group_metrics['mae'].mean():.2f}\")\n",
    "print(f\"  Best MAE:               {group_metrics['mae'].min():.2f}\")\n",
    "print(f\"  Worst MAE:              {group_metrics['mae'].max():.2f}\")\n",
    "\n",
    "# Top 5 worst groups\n",
    "print(f\"\\n  âš ï¸  TOP 5 WORST PERFORMING GROUPS:\")\n",
    "worst = group_metrics.nlargest(5, 'mae')[['group_id', 'actual_sum', 'pred_sum', 'mae']]\n",
    "for idx, row in worst.iterrows():\n",
    "    print(f\"     {row['group_id']}: MAE={row['mae']:.2f}, Actual={row['actual_sum']:.0f}, Pred={row['pred_sum']:.0f}\")\n",
    "\n",
    "# Top 5 best groups\n",
    "print(f\"\\n  âœ… TOP 5 BEST PERFORMING GROUPS:\")\n",
    "best = group_metrics.nsmallest(5, 'mae')[['group_id', 'actual_sum', 'pred_sum', 'mae']]\n",
    "for idx, row in best.iterrows():\n",
    "    print(f\"     {row['group_id']}: MAE={row['mae']:.2f}, Actual={row['actual_sum']:.0f}, Pred={row['pred_sum']:.0f}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. Feature Importance (à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¹„à¸«à¸™à¸ªà¸³à¸„à¸±à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”)\n",
    "# ============================================\n",
    "print(f\"\\n{'â”€'*60}\")\n",
    "print(f\"ğŸ” TOP 10 IMPORTANT FEATURES:\")\n",
    "print(f\"{'â”€'*60}\")\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "for idx, row in importance_df.head(10).iterrows():\n",
    "    bar_length = int(row['importance'] * 50)\n",
    "    bar = 'â–ˆ' * bar_length\n",
    "    print(f\"  {row['feature']:20s} {bar} {row['importance']:.4f}\")\n",
    "\n",
    "# à¸šà¸±à¸™à¸—à¸¶à¸ evaluation results\n",
    "group_metrics.to_csv('models/evaluation_per_group.csv', index=False)\n",
    "importance_df.to_csv('models/feature_importance.csv', index=False)\n",
    "print(f\"\\nâœ“ Saved: models/evaluation_per_group.csv\")\n",
    "print(f\"âœ“ Saved: models/feature_importance.csv\")\n",
    "\n",
    "# ============================================\n",
    "# 5. à¸ªà¸£à¸¸à¸›à¸œà¸¥à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“‹ EVALUATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  âœ“ Model Type:      XGBoost Regressor\")\n",
    "print(f\"  âœ“ Training Device: GPU\")\n",
    "print(f\"  âœ“ Training Time:   {training_time:.2f} seconds\")\n",
    "print(f\"  âœ“ MAE:             {mae:.4f}\")\n",
    "print(f\"  âœ“ RMSE:            {rmse:.4f}\")\n",
    "print(f\"  âœ“ RÂ²:              {r2:.4f}\")\n",
    "print(f\"  âœ“ MAPE:            {mape:.2f}%\")\n",
    "print(f\"  âœ“ Total Accuracy:  {(1 - abs(y_pred.sum() - y_test.sum()) / y_test.sum()) * 100:.1f}%\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf6c6804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”® GENERATING FUTURE FORECAST\n",
      "============================================================\n",
      "\n",
      "Forecasting:\n",
      "  â€¢ Period: 2025-12-01 00:00:00 to 2026-01-31 00:00:00\n",
      "  â€¢ Days: 62\n",
      "  â€¢ Groups: 952\n",
      "  â€¢ Total predictions: 59,024\n",
      "\n",
      "  Processing [100/952]...\n",
      "  Processing [200/952]...\n",
      "  Processing [300/952]...\n",
      "  Processing [400/952]...\n",
      "  Processing [500/952]...\n",
      "  Processing [600/952]...\n",
      "  Processing [700/952]...\n",
      "  Processing [800/952]...\n",
      "  Processing [900/952]...\n",
      "  Processing [952/952]...\n",
      "\n",
      "============================================================\n",
      "âœ… FORECAST COMPLETE\n",
      "============================================================\n",
      "  ğŸ“Š Total predictions:  59,024 rows\n",
      "  ğŸ“Š Total cases:        169,950\n",
      "  ğŸ“Š Avg per day:        2,741.1\n",
      "  ğŸ“ Saved to:           forecast_2025_2026.csv\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# FORECAST FUTURE (2025-12 to 2026-01)\n",
    "# ============================================\n",
    "print(f\"{'='*60}\")\n",
    "print(\"ğŸ”® GENERATING FUTURE FORECAST\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "future_dates = pd.date_range('2025-12-01', '2026-01-31', freq='D')\n",
    "all_groups = df['group_id'].unique()\n",
    "\n",
    "print(f\"Forecasting:\")\n",
    "print(f\"  â€¢ Period: {future_dates[0]} to {future_dates[-1]}\")\n",
    "print(f\"  â€¢ Days: {len(future_dates)}\")\n",
    "print(f\"  â€¢ Groups: {len(all_groups)}\")\n",
    "print(f\"  â€¢ Total predictions: {len(future_dates) * len(all_groups):,}\\n\")\n",
    "\n",
    "feature_cols = [\n",
    "    'month', 'day_of_month', 'day_of_week', 'is_weekend', \n",
    "    'is_december', 'year', 'rcode', 'aampur_clean', 'aplace_clean',\n",
    "    'group_mean', 'group_std', 'lag_1year', 'lag_2year', \n",
    "    'rolling_mean_7d', 'rolling_mean_14d'\n",
    "]\n",
    "\n",
    "forecasts = []\n",
    "for idx, group in enumerate(all_groups, 1):\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"  Processing [{idx}/{len(all_groups)}]...\")\n",
    "    \n",
    "    group_data = df[df['group_id'] == group].iloc[-1]\n",
    "    \n",
    "    for date in future_dates:\n",
    "        forecast_row = {\n",
    "            'adate': date,\n",
    "            'group_id': group,\n",
    "            'month': date.month,\n",
    "            'day_of_month': date.day,\n",
    "            'day_of_week': date.dayofweek,\n",
    "            'is_weekend': 1 if date.dayofweek >= 5 else 0,\n",
    "            'is_december': 1 if date.month == 12 else 0,\n",
    "            'year': 2026 if date.month == 1 else 2025,\n",
    "            'rcode': group_data['rcode'],\n",
    "            'aampur_clean': group_data['aampur_clean'],\n",
    "            'aplace_clean': group_data['aplace_clean'],\n",
    "            'group_mean': group_data['group_mean'],\n",
    "            'group_std': group_data['group_std'],\n",
    "            'lag_1year': 0,\n",
    "            'lag_2year': 0,\n",
    "            'rolling_mean_7d': group_data['group_mean'],\n",
    "            'rolling_mean_14d': group_data['group_mean']\n",
    "        }\n",
    "        forecasts.append(forecast_row)\n",
    "\n",
    "print(f\"  Processing [{len(all_groups)}/{len(all_groups)}]...\")\n",
    "\n",
    "# Predict\n",
    "forecast_df = pd.DataFrame(forecasts)\n",
    "X_forecast = forecast_df[feature_cols]\n",
    "\n",
    "forecast_df['predicted_cases'] = model.predict(X_forecast)\n",
    "forecast_df['predicted_cases'] = forecast_df['predicted_cases'].clip(lower=0).round()\n",
    "\n",
    "# Save\n",
    "output = forecast_df[['adate', 'rcode', 'aampur_clean', 'aplace_clean', 'predicted_cases']]\n",
    "output.to_csv('forecast_2025_2026.csv', index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"âœ… FORECAST COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  ğŸ“Š Total predictions:  {len(output):,} rows\")\n",
    "print(f\"  ğŸ“Š Total cases:        {output['predicted_cases'].sum():,.0f}\")\n",
    "print(f\"  ğŸ“Š Avg per day:        {output.groupby('adate')['predicted_cases'].sum().mean():,.1f}\")\n",
    "print(f\"  ğŸ“ Saved to:           forecast_2025_2026.csv\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8551f2",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# à¸§à¸´à¸˜à¸µà¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸à¸¥à¸±à¸šà¸¡à¸²à¹ƒà¸Šà¹‰\n",
    "# ============================================\n",
    "print(f\"{'='*60}\")\n",
    "print(\"ğŸ“– HOW TO LOAD MODEL\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\"\"\n",
    "# à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 1: à¹‚à¸«à¸¥à¸”à¸ˆà¸²à¸ JSON (à¹à¸™à¸°à¸™à¸³)\n",
    "loaded_model = xgb.XGBRegressor()\n",
    "loaded_model.load_model('models/xgboost_accident_model.json')\n",
    "\n",
    "# à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 2: à¹‚à¸«à¸¥à¸”à¸ˆà¸²à¸ Pickle\n",
    "import pickle\n",
    "with open('models/xgboost_accident_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# à¹ƒà¸Šà¹‰à¸—à¸³à¸™à¸²à¸¢\n",
    "predictions = loaded_model.predict(X_new)\n",
    "\"\"\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"ğŸ‰ ALL DONE!\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
